==========================================
 rmalloc: Relocatable Memory Allocator
==========================================
:Author: Mikael Jansson <mail@mikael.jansson.be>

.. contents::

Purpose
=======
To write and benchmark a special-purpose allocator that can compact its heap,
while still being fast enough for usage within the web browser Opera.

What
====
The purpose of rmalloc is to provide the ability to perform compacting of the
application heap, by making memory access indirect, e.g.::

    status_t rmalloc(memory_t **, ssize_t);
    status_t rmlock(memory_t *, void **);
    status_t rmunlock(memory_t *);
    status_t rmfree(memory_t *);

Example::

    #define WIDTH 100
    #define HEIGHT 100

    /* allocate memory for the bitmap */
    memory_t *bitmap;
    rmalloc(&bitmap, WIDTH*HEIGHT);

    /* get reference to actual memory */
    unsigned char *ptr;
    rmlock(bitmap, (void **)&ptr);

    /* draw pattern */
    for (y=0; y<HEIGHT; y++)
        for (x=0; x<WIDTH; x++)
            ptr[x + y*WIDTH] = x^y;

    /* release pointer and call auxillary code to do something with memory */
    rmunlock(bitmap);
    draw_bitmap(bitmap);

    /* done with the bitmap, release allocated memory */
    rmfree(bitmap);

Meta-How
==========
Three steps, of which the two last ones are iterative: 

#. Gather allocation statistics from real-world usage of Opera
#. Benchmark current allocators against rmalloc, by feeding data from the
   previous step into the allocators.
#. Tweak rmalloc.

Benchmarking
~~~~~~~~~~~~~~
Because of the difference in usage between rmalloc and standard allocators,
the standard allocators will be wrapped such that they can be plugged in as
direct replacements for rmalloc. 

How
====
Allocation status of a picee of memory is set in memory_block_t::used. An
allocation sets the value to true, a free sets it to false. 
Memory blocks are stored together with the allocated spcaes.  Memory handles
are currently also stored together with the user data. 

There are two options for giving back memory when freeing memory handles:

1. Add a member to memory_t, used, for next collection.
2. Reserve a place in RAM for storing only memory_ts, on the assumption that
   there will be a maximal umber of memory chunks allocated.

For #2, we need to know the typical memory size requested by the application.
In general, this is unknown, but as this allocator is going to be used in a
special-purpose setting, that value cna be found out.

Given the following variables.

:R: Total amount of memory currently available for use by the allocator
:M: Size of each piece of user-allocated memory
:D: Overhead (i.e. size of internal data sructures) associated with each
    user-allocated memory chunk.


2008-07-14
===========
Been sick for the past week, so the time schedule is delayed by one week.

Simple start, double-linked list stores the memory chunks and the info about
them. User-exposed structure keeps a pointer plus the number of locks held.
Lot of overhead currently, more efficient to store memory blocks as an array
to avoid the two pointers that make up the double-linked list.  Optionally,
could use a single-linked list.   without the link pointers, place the
structures at the very end of the memory and grow downwards, towards the
memory (i.e. stack).  this is better than placing them in the beginning,
because either space will be lost, or too little space is preserved and the
memory chunks have to be moved around.

a merge of two adjacent free blocks just increases the size of the first block
to the end of the second block, because memory blocks and chunks are located
together in memory. moving metadata out to a separate storage area would mean
it would have to work another way.

conventional allocators try to reuse memory as much as possible, and devise
clever algorithms to avoid fragmentation as much as possible.  An hypothesis
is that by disregarding everything "clever", and instead do compacting when
the heap is full (given a fixed-size heap), a higher level of efficiency can
be reached. especially when considered that the allocator could be given a
callback to be called when the heap has reached the lower and higher threshold
value needed to be able to perform compacting (when the end address of the
last memory chunk is too close to the end of the heap).

the compacting can be invoked automatically or manually, by the callback
indicating this at the lower threshold call.  if the manual compacting was
selected at the lower threshold but none made when the higher threshold
callback was invoked, an automatic compacting will be done.

banchmarking
~~~~~~~~~~~~~
benchmarking rmmalloc will be done by measuring how much memory can be
allocated before no more memory can be recovered.  when no memory can be
recovered depends on many things: 

* when alloc() fails, the callee could release other resources so another call
  to alloc() succeeds.
* this particular allocation isn't important, and later on in the cycle when
  possibly more memory has been given back to the allocator by the client
  application, the next alloc() might very well succeed. 

generally, it is up to the application to decide if a specific alloc()-fail
is catastrophic or not.  by using heuristics, the application can decide the
point of no return, i.e., where it doesn't make any sense to contirue trying
to allocate memory, as there will most likely not be any memory left in the
heap to allocate.

for benchmarking purposes, one way to perform this heuristics is to measure
when the number of NULL allocs is higher than the number of non-NULL allocs.
moreover, because of the indirection used by rmalloc, a regular allocator
needs to define the lock and unlock operations (as identity, but defined
nevertheless). finally, the purpose of rmalloc is to be efficient in
memory-constrained systems, so a fixed heap must be enforced.  Both rmalloc
and the other malloc (e.g. jemalloc, dlmalloc, ...) wil have to operate under
these constraints.

2008-07-20
==================
merging blocks on the free list is finished.  next up is performing
compacting.

Should the never-reuse-free-list method prove too slow, because compacting has
to happen very often, it's possible to use portions of the buddy allocator,
i.e.  the idea to sort the free list and keep the blocks on size boundaries,
that is, a list list with each node being double in size from the previous,
and each "bucket" being another pointer to a linked list with pointers to the
free slots.  then, free'd memory can be re-used, at the cost of splitting it
up into the part that's taken and the part that's free, creating an extra
memory block. a way to not have to do this is to use the entire slot, even
though it's larger than the requested amount of memory, yielding high internal
fragmentation.  without testing using proper data, it is not possible to
verify which method would yield the best results.

Compacting notification
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
after compacting, specify and implement the callbacks::

    threshold = {LOWER, HIGHER}
    status_t rmalloc_compact_notify(int threshold);

    typedef status_t (*notify)(int threshold) rmalloc_compact_notify_cb;

The callback determines if the compacting is to happen now by rmalloc, or
defer until later. A possible reason for defering the compacting to later is
that the program code could release soon release resources that would make
compating unnecessary. If the callback decides compacting should not be
performed at this moment, rmalloc will wait until the HIGHER threshold value
has been reached.

Typical usage::

    status_t notify(int threshold) {
        if (threshold == LOWER) {
            // we're soon about to release a bunch of resources,
            // so let's ask rmalloc not to compact just yet.
            // OR:
            // we are the middle of a time-sensitive operation, so please hold
            // off doing the compacting now and instead perform as late as
            // possible, either by ourselves invoking rmalloc_compact(), or by
            // automatic invocation by rmalloc (however, not any longer than
            // the point where threshold == HIGHER)
            // OR:
            // we will soon spend time in an idle loop, and it would be
            // beneficial to perform the compacting then instead of now.
            // we'll manually invoke rmalloc_compact()!

            return RM_NOT_YET;
        } else { // HIGHER
            // alright, compacting will happen now.
        }
    }

    rmalloc_register_fullness_notify(notify);

Strategies for compacting
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In this first pass, it will not be an incremental compactor.  The reason is
that compacting should happen when the client is otherwise idle and as such no
application time is wasted.

Memory block organization
~~~~~~~~~~~~~~~~~~~~~~~~~~
Instead of storing the memory blocks (i.e., everything but the storage
chunks), it might be desirable to store that data in a chunk of memory
allocated by the system-standard allocator (remember, rmalloc is not meant
as a replacement for regular malloc()/free() as it it does not have the same
semantics nor syntax, so client programs must be written specifically to
support it.)

2008-07-24
============
The first plan for doing compacting is to start at the beginning and move
forward in the list and move all used blocks to the beginning.

Happens like this::

    |free|      |free|       |used|       |used|
    |free|  =>  ------   =>  ------   =>  ------
    ------      |free|       |free|       |free|
    |used|      -----        -----        |free|
                |used|       |free|       

Before splitting, make sure there is enough room to fit in the new used
block. First, start merging all free blocks, and then go from the first free
block, and continue grabbing the first used block.  The free block must be
lagre enough to accomodate both the used block and the metadata for a new used
block. So, a split_block is neccessary to introduce first.
  
  free_size >= sizeof(memory_t)+sizeof(memory_block_t)
               + size of the used block

2008-07-25
===========
It's possible that by sharing structure between memory_block_t and
memory_t, organization would be a bit easier::

    typedef struct {
        uint8_t locks;
    } memory_t;

    struct memory_block_t {
        memory_t memory;
        void *ptr;
        uint8_t used : 1;
        size_t size;
        memory_block_t *previous;
        memory_block_t *next;
    };

The internal protocol would have to cast the public structure into the
private, which gives you a clean public API at the cost of an internal cast
(the macro MEMORY_TO_BLOCK(x))

2008-07-30
===========
The root block is allocated such that it has a NULL pointer and size == 0.

2008-08-18
===========
It's probably useful to be able to specify in rmalloc_init() what memory to
use, e.g.::

    rmalloc_init(RM_USER_SUPPLIED, our_heap);
    rmalloc_init(RM_SYSTEM_MEMORY, NULL);
    /* XXX: merged w/ RM_USER_SUPPLIED? */
    rmalloc_init(RM_SYSTEM_MALLOC, NULL);


2008-08-19: mb_move()
=========================
mb_move(free_block, used_block) moves the data within used_block to
free_block. free_block.size > used_block.size => free_block is split into
used_block.size and the leftover block is attached directly to the free block
(by means of mb_shrink().)   It will also have to update the user-level
pointers (i.e., memory_t) somehow.. this can be done in a function above
mb_move(), or inside. For now, it's better to keep it all in one atomic
operation.

With the curent layout memory_block_t followed directly by the data pointed to
by ptr (why does the ptr even exist, then?), it's impossible to change
location of memory blocks without updating the user code, if the layout is to
stay the same.

Two options, one with a time penalty on compacting (ptr movable), and one with
an extra indirection (hashed ...) causing a time penalty on handle locking and
space penalty on the hash table used.

ptr movable
~~~~~~~~~~~~~~~
Don't force a layout where the data pointed to by "ptr" has to directly follow
the memory_block_t, i.e. This affects the shrink and merge operations which
assume that layout to shrink/merge adjacent blocks

Trade-offs
-----------
This will cause the merge operation (but not shrink) to become slower, as
finding the adjacent free block will not be as easy as following the linked
list and check for used==0.  In the future, one idea is to keep the
memory_block_t data separate from the pointers anyway, for example at the end
of the heap and overallocate to make sure the heap doesn't have to be resized
all the time when adding each block (24 bytes).  Should this be implemented,
the former approach would be quite natural.

Hashed memory_t > memory_block_t > ptr
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Underlying memory_block_ts can be moved around freely, but the price to pay is
a memory_t -> memory_block_t mapping, and that way only the entry in the
mapping is updated when memory is moved around, and so the current layout can
be kept.

Trade-offs
~~~~~~~~~~~
The extra hash table required to lookup memory_block_t (ptr) will cause a
second lookup in the hash for each lock(), and in addition the hash table will
consume memory. This could possible be kept small, for example an uint16_t
(for a max of 65536 objects, though) as an offset into the list of
memory_block_ts.

Conclusion
~~~~~~~~~~~
Having an extra lookup is probably not a viable option, as the point of the
allocator is to be memory efficient, and the hash table consumes a lot of
memory, N * sizeof(void*), where N is the number of objects. Also, it will
need to be located somewhere in memory. If kept at the beginning (before the
blocks), it will have to be overallocated (leading to internal fragmentation)
in order to not move the real memory blocks to move around muchh.

Extra
~~~~~
Looking at this closer, I see that there is already a hash table in place,
because of "ptr" -- it is not neccessary to have in the structure if the
user memory is located directly after m_b_t in the heap. One solution, placing
all haeders at the end of the heap, causes problems if we want to shrink/grow
the heap, because the headers are fixed in place because of the userspace
mapping.

::


    (23:15:33) Jansson: Jo, jag kan lägga den i början på heapen och ha lite
    intern fragmentering, tills jag når objektdelen av heapen, och då får jag
    flytta dem.
    (23:15:50) Peter Wallman: det där var jag inte med på?
    (23:16:34) Jansson: jag använder, säg, heap[0] till
    heap[1024*sizeof(memory_block_t)] för att placera headers på, och direkt efter
    kommer riktig data.
    (23:16:54) Jansson: sedan när någon allokerat >1024 block får man så snällt
    flytta allt minne för att göra plats för N*2 headers.
    (23:17:14) Peter Wallman: ah, du kan ju kompaktera fram mer plats, ja :)
    (23:17:19) Jansson: Jess!
    (23:17:37) Peter Wallman: tja, varför inte.
    (23:18:48) Jansson: Det blir.. skavise..
    bytes_occupied_by_objects/(new_header_count-old_header_count)*sizeof(header)
    memcpy()s

Something like this::

    memcpy(src+size-diff, src+size, diff)
    memcpy(src+size-diff*2, src+size-diff, diff)
    memcpy(src+size-diff*3, src+size-diff*2, diff)


The alternative is to have the headers in a separate heap. Is it worth it?
The point was to avoid fragmentation. That heap would increase a lot. Having
headers in the same heap as the memory would on one be a bit slower when we
run out of header slots, but on the other hand be entirely self-contained.
Requires measurement!  One misadvantage of the separate heap is that they
might run out of memory at different times.

::

    en annan idé jag har, kommer inte ihåg om jag berättat om den, är att jag ger
    ifrån mig en notifiering till användaren  om att "nu borde jag börja
    kompaktera, ska jag göra det?" där användaren kan säga "nah." och tömma lite
    cacher och sådär.  om användaren ändå inte gör något så kommer rmalloc ge
    ifrån sig en till notifiering, när det är så lite minne kvar att man /måste/
    kompaktera nu för att det ska gå alls, som inte är valfri.

    är det användbart ö.h.?
    (23:34:47) Jansson: (t.ex. kan man tänka sig att Opera säger "nej, kompaktera
    inte nu" när något tungt sker, och så triggar Opera själv det i runslice.)
    (23:35:31) Peter Wallman: spontant: ja
    (23:35:45) Jansson: spontant är det också min tanke, men praktiskt? :-)
    (23:36:00) Jansson: det jag tänkte med det här var att helt och hållet
    amortera undan kompakteringen i operas idleloop.
    (23:36:06) Jansson: och få en raketsnabb allokator.
    (23:36:27) Peter Wallman: ja, det kan vara en ide
    (23:36:37) Jansson: eventuellt göra en inkrementell kompaktering, t.om.
    (23:36:40) Peter Wallman: så fort vi idlar så kör vi lite kompaktering, typ
    (23:36:42) Peter Wallman: precis

2008-08-26: Meeting with Koen
==================================
When merging the free blocks on the (implicit in the haeders) free lists, a
O(n^2) operation is done to find adjacent blocks, that is:

    for h in headers:
        for h' in headers:
            if adjacent(h, h'):
                merge(h, h')

Instead, store in the datablock a pointer to the adjacent free data block's
header. That way, finding adjacent blocks is a simple matter of iterating the
list.  If no adjacent block, set to NULL to distinguish left-overs in the
datablock from an actual pointer.

A tricky situation (relationship by character, not by being on the same line,
\* denotes used block) that can come up if free data blocks are re-used in
allocations, i.e. yielding a non-increasing data block order with regard to
the headers::

    header  data
     A       A -> B         (store a pointer to B's header)
     B       B -> null      (no adjacent free block)
     C      *D              (used block)
     D       E -> G         (store a pointer to G's header)
     E       G -> null      (no adjacent free block)
     F      *F              (used block)
     G       C -> H         (store a pointer to H's header)
     H       H -> null      (no adjacent free block)
     .      *.
     .       .

Merging
~~~~~~~~~~~~~~~~
This is how a faulty algorithm, and my first try, looks like:

1. Start at header(A): follow link to B, merge A and B.
2. Continue at last used block, header(B) and go to next header (C)
3. Follow link to C, merge C and H.
4. Continue at last used block, header(H) and go to next header (.)
5. ...

The free pair E, G isn't considered!

Thus, each time a link is followed, the "next header" can only be set if the
free pair's headers are monotonically increasing, i.e. A, B, C, D, ... If
greater jumps are made, such as C -> H, "next header" must be D! First sketch
of the merge operation::

    current = root(headers)
    while current /= end(headers):
        if free(h):
            h = current
            increasing = T
            while h /= end(headers) and free(h):
                h' = header(data(h))
                when free(h'):
                    h'' = merge(h, h')
                    if increasing-by-1(h, h'):
                        when increasing:
                            current = h'
                        h = h'

                    else:
                        increasing = NIL
        else:
            current = next(current)
                    
In plain text: when following a link, update the current (top-level) index
into the header list as long as the linked-to blocks are increasing by one
header at the time. As soon as blocks are not increasing-by-1, stop updating
the current pointer. 

XXX: Set the header's data block pointer to NULL for the (now unused) blocks.
This should probably be done in the merge() operation.

Trace of the merge given the table above::

    current = A
    h = A
    h' = header(data(A)) = B
    free(A) and free(B) =>
        h'' = merge(A, B)
        increasing-by-1(A, B) =>
            current = B
            h = h'
    B /= end(headers) and free(B) =>
        h' = header(data(B)) == NIL
    current = next(current) = C
    h = C
    C /= end(headers) and free(C) =>
        h' = header(data(C)) == H =>
            h'' = merge(C, H)
            increasing-by-1(C, H) == NIL =>
                increasing = NIL
    current = next(current) = D
            

Discussion
-----------------
Ends up visiting (some) blocks twice, but hopefully the usual case won't
have many jumps, which should be possible most of the time: directly after
compacting blocks can be arranged to fit the order of headers, but after some
time of re-using free data blocks, this order cannot be guaranteed.

Worst-case behaviour is illustrated by the following merge sequence: 15
headers, numbered H1-H15 with corresponding free data blocks D1-D15. Note that
non-adjacent header blocks can very well be pointing at adjacent data blocks!
In particular, the data block layout looks like this (line-breaking to make it
easier to read)::

    [D1][D5][D10]
    [used]
    [D2][D6][D11]
    [used]
    [D3][D7][D12]
    [used]
    [D4][D8][D13]
    [used]
    [D5][D9][D14]
    [used]
    [D15]

The used blocks are not important.

1. Starting with H1, the next header block will be H2. (as header blocks H1
2. The process is repeated for H3 (6, 11) and H4 (7, 12).
3. When moving forward to H5, the data block's next free data block pointer is
   NULL, same for 6 and 7.  
4. For 8 and 9, proceed with the merge as described in #1.
5. Blocks 10, 11, 12, 13 and 14 have already been visited.
6. Finally, block 16.

The total number of operations is 3 x (3 + 2) = 15, i.e. O(n).

It is probably a good idea to re-use free blocks, even though it means a
slightly slower malloc (to find a free data block large enough to hold the
requested size), but compacting should happen less frequently. Moreover,
large enough can very well be the exact size or exact size +
(>= sizeof(memory_block_t*)) to fit the pointer to a possible free adjacent data
block's header.

2008-08-29: free() and generations
==================================

Keeping the free list in the data blocks requires the initial free list to be
setup somewhere, and the natural location to do that is in free(). The data
blocks need to have both a pointer back to its header, as well as a pointer to
the next free data block. So, why not perform a merge of the neighbors? This
can be done in two ways, one where free() is O(1) in time complexity and one
where free() is O(n) (where n is the number of free blocks) to find the block,
plus the time to merge two blocks. (Which is also O(1).)  

One piece of information that both approaches need to store is a pointer to
the next free data block, or finding a block will be O(n) for n = total block
count. The data blocks need to be inserted inte the list in order.

Pointer at start
~~~~~~~~~~~~~~~~
When a data block is freed, the corresponding sets the used flag to 0, and a
pointer back to the header is stored at the first of the block.

Pointers at both boundaries
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The initial setup is the same as for as the above approach, but with the
addition of an extra header pointer at the end of the data block.
The merge operation is done by looking at the current data block's starting
address minus sizeof(void*) and verifying that the address is in range to be a
header block, and then following the header block's data block pointer to make
sure it's unused and pointing to that block's starting addres.  Similarly,
checking for an unused block is done by looking past the end of the current
data block. This makes the merge operation O(1), but at the expense of having
to store one extra pointer.

Discussion
~~~~~~~~~~
By adding one pointer, the merge operation can be performed rather fast,
but with merge() taking O(n) in time, it is not certain that a 50% increase of
the minimum block size (two pointers to three) is worth the reduction in time
(a cheap operation in each free() vs a O(n) (n=free block count) block-merge
in compact()).

A further optimization can be done by making the assumption that there will
never be more than 2^24 blocks (either used or free) at any one time, and as
such transforming the direct header block pointer to 3-byte long offset into
the header block list.  The same thing can be done for the free list, by
pointing at the free block's header instead of the block itself.

It turns out that this approach makes it possible to rid of the pointor to the
block's header altogether, as it will always be known by the previous block.
Moreover, as blocks most likely should be aligned on a memory boundary (not
addressed here), it's possible that blocks need to be 8 bytes anyway, and
saving 4 bytes on the extra pointer is pointless. In that case, the list could
be made doubly linked for a slightly quicker search (e.g. by keeping
references to 25%, 50% and 75% into the list and perform binary searches in
those ranges to find the location for the insertion.)


2008-08-30: Compacting policies
====================================

It is possible to make smaller degrees of compacting without much overhead, by
making note of the state of the free and used blocks. This gives the
possibility to quickly ("cheaply") move around a few blocks to reduce
fragmentation without invoking a full compact operation.

Three policies are devised: moving a block yields large amounts of memory;
moving a block yields less memory but doesn't invoke compact; full compact.
It's a scale starting from hinting to forceful, and the user is given a choice
through callbacks to let the allocator know what to do (either by doing what
the allocator believes should be done, or by postponing the requested action
into a later point in time when it is more suitable for the client
application, e.g. an idle loop.)

Large Free
~~~~~~~~~~~~~
Keep track of the block whose sum of its own size and direct-neighbor free
blocks is the largest, and the free blocks large enough to hold the used block
itself (i.e. without the direct-neighbor free blocks). When the count of such
free blocks becomes 1, flag a "cleanup" operation.

Expressed in another way (free for blocks marked free, used for blocks marked
used)::

    lu = exists u  <- used :
        forall b <- used \ u :
          size(u)+size(direct-neighbors(u)) >= 
          size(b)+size(direct-neighbors(b)))

    lfs = forall f <- free : size(f) => size(lu)

    |lfs| > 1 --> |lfs| == 1 ==> flag(cleanup)

Small Free
~~~~~~~~~~~~~
Similar to the Large version in implementation, but keeping track of the
smallest free blocks, denoting the last possible "cheap" operation that can
free up some memory instead of performing a compacting::

    su = exists u <- used :
        forall b <- used \ u :
            size(u)+size(direct-neighbors(b)) <=
            size(b)+size(direct-neighbors(b))

    sfs = forall f <- free : size(f) >= size(su)
    
    |sfs| > 1 --> |sfs| == 1 ==> flag(last-chance)

Force Compact
~~~~~~~~~~~~~
This is automatically entered if the user wanted to skip the last-chance when
it was flagged. 

XXX: Does the user really want to know about the cleanup and last-chance
states? Shouldn't the allocator always use those if possible? If so, why would
ever a force-compact happen?

Compacting aims to be incremental, so blocks are moved from the top of the
heap into the available holes. If no holes are available, blocks are moved
around to create space (XXX: using which strategy?) to fit the block with the
highest memory address. This is done so the heap can be shrinked if needed, at
any given point in time.

.. vim:syntax=rst:ts=4:sw=4

