==========================================
 rmalloc: Relocatable Memory Allocator
==========================================
:Author: Mikael Jansson <mail@mikael.jansson.be>

.. contents::

Purpose
=======
To write and benchmark a special-purpose allocator that can compact its heap,
while still being fast enough for usage within the web browser Opera.

What
====
The purpose of |rmalloc| is to provide the ability to perform compacting of the
application heap, by making memory access indirect, e.g.::

    status_t rmalloc(memory_t **, ssize_t);
    status_t rmlock(memory_t *, void **);
    status_t rmunlock(memory_t *);
    status_t rmfree(memory_t *);

Example::

    #define WIDTH 100
    #define HEIGHT 100

    /* allocate memory for the bitmap */
    memory_t *bitmap;
    rmalloc(&bitmap, WIDTH*HEIGHT);

    /* get reference to actual memory */
    unsigned char *ptr;
    rmlock(bitmap, (void **)&ptr);

    /* draw pattern */
    for (y=0; y<HEIGHT; y++)
        for (x=0; x<WIDTH; x++)
            ptr[x + y*WIDTH] = x^y;

    /* release pointer and call auxillary code to do something with memory */
    rmunlock(bitmap);
    draw_bitmap(bitmap);

    /* done with the bitmap, release allocated memory */
    rmfree(bitmap);

Meta-How
==========
Three steps, of which the two last ones are iterative: 

#. Gather allocation statistics from real-world usage of Opera
#. Benchmark current allocators against |rmalloc|, by feeding data from the
   previous step into the allocators.
#. Tweak |rmalloc|.

Benchmarking
~~~~~~~~~~~~~~
Because of the difference in usage between |rmalloc| and standard allocators,
the standard allocators will be wrapped such that they can be plugged in as
direct replacements for |rmalloc|. 

How
====
Allocation status of a picee of memory is set in memory_block_t::used. An
allocation sets the value to true, a free sets it to false. 
Memory blocks are stored together with the allocated spcaes.  Memory handles
are currently also stored together with the user data. 

There are two options for giving back memory when freeing memory handles:

1. Add a member to memory_t, used, for next collection.
2. Reserve a place in RAM for storing only memory_ts, on the assumption that
   there will be a maximal umber of memory chunks allocated.

For #2, we need to know the typical memory size requested by the application.
In general, this is unknown, but as this allocator is going to be used in a
special-purpose setting, that value cna be found out.

Given the following variables.

:R: Total amount of memory currently available for use by the allocator
:M: Size of each piece of user-allocated memory
:D: Overhead (i.e. size of internal data sructures) associated with each
    user-allocated memory chunk.


2008-07-14
===========
Been sick for the past week, so the time schedule is delayed by one week.

Simple start, double-linked list stores the memory chunks and the info about
them. User-exposed structure keeps a pointer plus the number of locks held.
Lot of overhead currently, more efficient to store memory blocks as an array
to avoid the two pointers that make up the double-linked list.  Optionally,
could use a single-linked list.   without the link pointers, place the
structures at the very end of the memory and grow downwards, towards the
memory (i.e. stack).  this is better than placing them in the beginning,
because either space will be lost, or too little space is preserved and the
memory chunks have to be moved around.

a merge of two adjacent free blocks just increases the size of the first block
to the end of the second block, because memory blocks and chunks are located
together in memory. moving metadata out to a separate storage area would mean
it would have to work another way.

conventional allocators try to reuse memory as much as possible, and devise
clever algorithms to avoid fragmentation as much as possible.  An hypothesis
is that by disregarding everything "clever", and instead do compacting when
the heap is full (given a fixed-size heap), a higher level of efficiency can
be reached. especially when considered that the allocator could be given a
callback to be called when the heap has reached the lower and higher threshold
value needed to be able to perform compacting (when the end address of the
last memory chunk is too close to the end of the heap).

the compacting can be invoked automatically or manually, by the callback
indicating this at the lower threshold call.  if the manual compacting was
selected at the lower threshold but none made when the higher threshold
callback was invoked, an automatic compacting will be done.

banchmarking
~~~~~~~~~~~~~
benchmarking rmmalloc will be done by measuring how much memory can be
allocated before no more memory can be recovered.  when no memory can be
recovered depends on many things: 

* when alloc() fails, the callee could release other resources so another call
  to alloc() succeeds.
* this particular allocation isn't important, and later on in the cycle when
  possibly more memory has been given back to the allocator by the client
  application, the next alloc() might very well succeed. 

generally, it is up to the application to decide if a specific alloc()-fail
is catastrophic or not.  by using heuristics, the application can decide the
point of no return, i.e., where it doesn't make any sense to contirue trying
to allocate memory, as there will most likely not be any memory left in the
heap to allocate.

for benchmarking purposes, one way to perform this heuristics is to measure
when the number of NULL allocs is higher than the number of non-NULL allocs.
moreover, because of the indirection used by rmalloc, a regular allocator
needs to define the lock and unlock operations (as identity, but defined
nevertheless). finally, the purpose of rmalloc is to be efficient in
memory-constrained systems, so a fixed heap must be enforced.  Both rmalloc
and the other malloc (e.g. jemalloc, dlmalloc, ...) wil have to operate under
these constraints.

2008-07-20
==================
merging blocks on the free list is finished.  next up is performing
compacting.

Should the never-reuse-free-list method prove too slow, because compacting has
to happen very often, it's possible to use portions of the buddy allocator,
i.e.  the idea to sort the free list and keep the blocks on size boundaries,
that is, a list list with each node being double in size from the previous,
and each "bucket" being another pointer to a linked list with pointers to the
free slots.  then, free'd memory can be re-used, at the cost of splitting it
up into the part that's taken and the part that's free, creating an extra
memory block. a way to not have to do this is to use the entire slot, even
though it's larger than the requested amount of memory, yielding high internal
fragmentation.  without testing using proper data, it is not possible to
verify which method would yield the best results.

Compacting notification
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
after compacting, specify and implement the callbacks::

    threshold = {LOWER, HIGHER}
    status_t rmalloc_compact_notify(int threshold);

    typedef status_t (*notify)(int threshold) rmalloc_compact_notify_cb;

The callback determines if the compacting is to happen now by rmalloc, or
defer until later. A possible reason for defering the compacting to later is
that the program code could release soon release resources that would make
compating unnecessary. If the callback decides compacting should not be
performed at this moment, rmalloc will wait until the HIGHER threshold value
has been reached.

Typical usage::

    status_t notify(int threshold) {
        if (threshold == LOWER) {
            // we're soon about to release a bunch of resources,
            // so let's ask rmalloc not to compact just yet.
            // OR:
            // we are the middle of a time-sensitive operation, so please hold
            // off doing the compacting now and instead perform as late as
            // possible, either by ourselves invoking rmalloc_compact(), or by
            // automatic invocation by rmalloc (however, not any longer than
            // the point where threshold == HIGHER)
            // OR:
            // we will soon spend time in an idle loop, and it would be
            // beneficial to perform the compacting then instead of now.
            // we'll manually invoke rmalloc_compact()!

            return RM_NOT_YET;
        } else { // HIGHER
            // alright, compacting will happen now.
        }
    }

    rmalloc_register_fullness_notify(notify);

Strategies for compacting
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In this first pass, it will not be an incremental compactor.  The reason is
that compacting should happen when the client is otherwise idle and as such no
application time is wasted.

Memory block organization
~~~~~~~~~~~~~~~~~~~~~~~~~~
Instead of storing the memory blocks (i.e., everything but the storage
chunks), it might be desirable to store that data in a chunk of memory
allocated by the system-standard allocator (remember, rmalloc is not meant
as a replacement for regular malloc()/free() as it it does not have the same
semantics nor syntax, so client programs must be written specifically to
support it.)

2008-07-24
============
The first plan for doing compacting is to start at the beginning and move
forward in the list and move all used blocks to the beginning.

Happens like this::

    |free|      |free|       |used|       |used|
    |free|  =>  ------   =>  ------   =>  ------
    ------      |free|       |free|       |free|
    |used|      -----        -----        |free|
                |used|       |free|       

Before splitting, make sure there is enough room to fit in the new used
block. First, start merging all free blocks, and then go from the first free
block, and continue grabbing the first used block.  The free block must be
lagre enough to accomodate both the used block and the metadata for a new used
block. So, a split_block is neccessary to introduce first.
  
  free_size >= sizeof(memory_t)+sizeof(memory_block_t)
               + size of the used block

2008-07-25
===========
It's possible that by sharing structure between memory_block_t and
memory_t, organization would be a bit easier::

    typedef struct {
        uint8_t locks;
    } memory_t;

    struct memory_block_t {
        memory_t memory;
        void *ptr;
        uint8_t used : 1;
        size_t size;
        memory_block_t *previous;
        memory_block_t *next;
    };

The internal protocol would have to cast the public structure into the
private, which gives you a clean public API at the cost of an internal cast
(the macro MEMORY_TO_BLOCK(x))

2008-07-30
===========
The root block is allocated such that it has a NULL pointer and size == 0.

2008-08-18
===========
It's probably useful to be able to specify in rmalloc_init() what memory to
use, e.g.::

    rmalloc_init(RM_USER_SUPPLIED, our_heap);
    rmalloc_init(RM_SYSTEM_MEMORY, NULL);
    /* XXX: merged w/ RM_USER_SUPPLIED? */
    rmalloc_init(RM_SYSTEM_MALLOC, NULL);


2008-08-19
===========
mb_move(free_block, used_block) moves the data within used_block to
free_block. free_block.size > used_block.size => free_block is split into
used_block.size and the leftover block is attached directly to the free block
(by means of mb_shrink().)   It will also have to update the user-level
pointers (i.e., memory_t) somehow.. this can be done in a function above
mb_move(), or inside. For now, it's better to keep it all in one atomic
operation.


|rmalloc| replace:: ``rmalloc``

.. vim:syntax=rst:ts=4:sw=4

